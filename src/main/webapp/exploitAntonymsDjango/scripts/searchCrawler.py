import urllib
from bs4 import BeautifulSoup as soup
import requests
import webbrowser
import csv
import antGalore3
import itertools
import os
import json
import sys

if len(sys.argv) != 2:
    print("Usage: python3 searchCrawler.py confFile")
    sys.exit(1)

# open CSV file for writing, using DictWriter so will need DictReader later
with open('resultsLedger.csv', 'w', newline='') as csvFile:
    fieldNames = ['filename', 'title', 'snippet', 'href']
    # original order and searched query can be inferred by the filename (webpage)
    theWriter = csv.DictWriter(csvFile, fieldnames = fieldNames)

    theWriter.writeheader()

    # import conf file and load json data
    jsonData = json.loads(open(sys.argv[1]).read())
    queriesFile = jsonData["queriesCSVfile"]
    numOfRes = jsonData["numOfResPerQuery"]
    DLfolder = jsonData["path2folder"]
    maxFileSize = jsonData['maxFileSize']
    minFileSize = jsonData['minFileSize']
    dlAnyway = jsonData['dlAnyway']

    # open the csv file with the queries for reading
    original_queries_f = open(queriesFile, 'r')
    original_queries_reader = csv.reader(original_queries_f)
    
    for row in original_queries_reader:
        originalQuery = row[0]
        
        print('Extrapolating original query: ' + row[0])

        antonyms = antGalore3.ants(originalQuery)
        
        # dict of all relavent words
        nQueries = {}
        
        for word in originalQuery.split(' '):
            nQueries[word] = []
            nQueries[word].append(word)
        
        for word in antonyms:
            if antonyms[word] != []:
                for newWord in antonyms[word]:
                    nQueries[word].append(newWord)
        
        # iterate based on query, dictionary doesn't keep order
        
        # list of all relavent words in order
        queriesExtra = []
        for word in originalQuery.split(' '):
            queriesExtra.append(nQueries[word])

        # list of all possible extrapolated queries based on original query
        possibleQueries = list(itertools.product(*queriesExtra))
        
        for queryList in possibleQueries:
            query = ' '.join(queryList)
            query = urllib.parse.quote_plus(query)

            print('')
            print('Googling query: ' + query)

            # base google search url + query + number of results we want
            url = 'https://google.com/search?q=' + query + '&num=' + str(numOfRes + 50) # get some results more than we need

            # get response from google
            response = requests.get(url)

            # use beautiful soup to process the results page
            web_soup = soup(response.text, 'html.parser')

            
            # for each result in page, keep important information
            counter = 1

            # dict of downloaded pages
            pageDict = {}
            for res in web_soup.find_all(class_='g'):
                #print('-----')
                if res.h3 and res.h3.a:
                    href = res.h3.a['href'][7:]
                    # let's unwrap useful info
                    if href.startswith('h'):
                        ends = href.find('&')
                        href = href[:ends]
                        title = res.h3.text
                        snippet = res.find(class_='st').text

                    if href[0:4] != 'http':
                        href = 'http://' + href

                    # pageDict is a dictionary of different queries pointing
                    # to the same page
                    # href key
                    # all queries resulting in href (could be duplicates)
                    if href in pageDict:
                        pageDict[href] = pageDict[href] + ' ' + query
                        continue

                    pageDict[href] = query 

                    # download the landing page of the result and write it to a file
                    #print('Downloading: ' + href)
                    # get the extension if available, assumed html otherwise
                    site, ext = os.path.splitext(href)

                    # file types we are interested in
                    # Maybe try requests.head but it doesn't seem reliable enough
                    if ext == '.htm' or ext == '.ppt' or ext == '' or ext == '.html' or ext == '.pdf' or ext == '.docx' or ext == '.doc' or ext == '.txt':
                        try:
                            new_response = requests.get(href, stream=True, timeout=3.05)
                            if int(new_response.headers['content-length']) > minFileSize and int(new_response.headers['content-length']) < maxFileSize: # if min < file < MAX
                                new_response = requests.get(href, timeout=3.05)
                            else:
                                raise AssertionError('The file was either too big or too small.')
                        except KeyError as err:
                            #print('Content-length key not found. Downloading anyway: ' + str(dlAnyway))
                            if dlAnyway:
                                new_response = requests.get(href, timeout=3.05)
                            else:
                                continue
                        except AssertionError as err:
                            #print('Did not file at: ' + href + ' because size was: ' + new_response.headers['content-length'] + 'bytes')
                            continue
                        except Exception as exc:
                            #print('Got an exception of type: ' + str(type(exc)))
                            continue
                    else:
                        continue
                    #print('#' + str(counter) + ' Downloaded.')

                    if ext == '':
                        ext = '.html'

                    webPageFileName = DLfolder + '/' + query + str(counter) + ext

                    plainText = True
                    if ext == '.ppt' or ext == '.pdf' or ext == '.docx' or ext == '.doc':
                        plainText = False
                        print('Ext: ' + ext)
                    try:
                        if plainText:
                            new_file = open(webPageFileName, 'w')
                            new_file.write(new_response.text)
                        else:
                            new_file = open(webPageFileName, 'wb')
                            new_file.write(new_response.content)
                        new_file.close()
                    except:
                        print('Could not write to file from: ' + href)
                        continue
                            
                    # write new row to csv file
                    theWriter.writerow({'filename' : webPageFileName, 'title' : title, 'snippet' : snippet, 'href' : href})
                            
                    counter = counter + 1
                    print('.', end='', flush=True)
                    if counter > numOfRes:
                        break
                else:
                    continue
    original_queries_f.close()
    new_file = open('pageDict.json', 'w')
    new_file.write(json.dumps(pageDict))
    new_file.close()
