import urllib
from bs4 import BeautifulSoup as soup
import requests
import webbrowser
import csv
import antGalore3
import itertools
import os
#import json

# open CSV file for writing, using DictWriter so will need DictReader later
with open('resultsLedger.csv', 'w', newline='') as csvFile:
    fieldNames = ['filename', 'title', 'snippet', 'href']
    # original order and searched query can be inferred by the filename (webpage)
    theWriter = csv.DictWriter(csvFile, fieldnames = fieldNames)

    theWriter.writeheader()

    # open the csv file with the queries for reading
    original_queries_f = open('queries.csv', 'r')
    original_queries_reader = csv.reader(original_queries_f)
    
    for row in original_queries_reader:
        originalQuery = row[0]
        
        print('Extrapolating original query: ' + row[0])

        antonyms = antGalore3.ants(originalQuery)
        
        # dict of all relavent words
        nQueries = {}
        
        for word in originalQuery.split(' '):
            nQueries[word] = []
            nQueries[word].append(word)
        
        for word in antonyms:
            if antonyms[word] != []:
                for newWord in antonyms[word]:
                    nQueries[word].append(newWord)
        
        # iterate based on query, dictionary doesn't keep order
        
        # list of all relavent words in order
        queriesExtra = []
        for word in originalQuery.split(' '):
            queriesExtra.append(nQueries[word])
        
        # list of all possible extrapolated queries based on original query
        possibleQueries = list(itertools.product(*queriesExtra))
        
        for queryList in possibleQueries:
            query = ' '.join(queryList)
            query = urllib.parse.quote_plus(query)

            print('Googling query: ' + query)

            # number of results we want
            numOfRes = 100

            # base google search url + query + number of results we want
            url = 'https://google.com/search?q=' + query + '&num=' + str(numOfRes)

            # get response from google
            response = requests.get(url)

            # use beautiful soup to process the results page
            web_soup = soup(response.text, 'html.parser')

            
            # for each result in page, keep important information
            counter = 1

            # dict of downloaded pages
 #           pageDict = {}
            for res in web_soup.find_all(class_='g'):
                #print('-----')
                if res.h3 and res.h3.a:
                        href = res.h3.a['href'][7:]
                        # let's unwrap useful info
                        if href.startswith('h'):
                            ends = href.find('&')
                            href = href[:ends]
                            title = res.h3.text
                            snippet = res.find(class_='st').text
                            #print('Title: ' + title)
                            #print('Link: ' + href)
                            #print('Snippet: ' + snippet)

  #                          if href in pageDict:
   #                             pageDict[href] = pageDict[href] + 1
    #                            continue

     #                       pageDict[href] = 1 

                            # download the landing page of the result and write it to a file
                            print('Downloading: ' + href)
                            # get the extension if available, assumed html otherwise
      #                      site, ext = os.path.splitext(href)
                            # file types we are interested in
                            # Maybe try requests.head but it doesn't seem reliable enough
       #                     new_response = None 
        #                    if ext == '' or ext == '.html' or ext == '.pdf' or ext == '.docx' or ext == '.doc' or ext == '.txt':
                            try:
                                new_response = requests.get(href)
                            except:
                                print('Did not download page at: ' + href)
                                continue
                            print('#' + str(counter) + ' Downloaded: ' + href)

                            #if ext == '':
                             #   ext = '.html'

                            #webPageFileName = 'DLed_webpages/' + query + str(counter) + ext
                            webPageFileName = 'DLed_webpages/' + query + str(counter) + '.html'
                            new_file = open(webPageFileName, 'w')
                            #print('Trying to write file: ' + href)
                            new_file.write(new_response.text)
                            new_file.close()
                            
                            # write new row to csv file
                            theWriter.writerow({'filename' : webPageFileName, 'title' : title, 'snippet' : snippet, 'href' : href})
                            
                            counter = counter + 1
                        else:
                            continue
    original_queries_f.close()
#    new_file = open('pageDict.json', 'w')
 #   new_file.write(json.dumps(pageDict))
  #  new_file.close()
