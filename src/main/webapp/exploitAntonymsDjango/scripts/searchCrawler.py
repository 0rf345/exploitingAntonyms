import urllib
from bs4 import BeautifulSoup as soup
import requests
import webbrowser
import csv
import antGalore3
import itertools
import os
import json
import sys
import time
import copy

# when was the last time we asked google for a query
lastCall = 0 

if len(sys.argv) != 2:
    print("Usage: python3 searchCrawler.py confFile")
    sys.exit(1)

# import conf file and load json data
jsonData = json.loads(open(sys.argv[1]).read())
queriesFile = jsonData["queriesCSVfile"]
numOfRes = jsonData["numOfResPerQuery"]
DLfolder = jsonData["path2folder"]
maxFileSize = jsonData['maxFileSize']
minFileSize = jsonData['minFileSize']
chunkSize = jsonData['chunkSize']
defaultExt = '' # no extension, tika will figure it out and convert it
resultsLedgerPath = jsonData['resultsLedger']
pageDictPath = jsonData['pageDict']
supportedExtensions = jsonData['extensions']
filesDownloadedPath = jsonData['filesDownloaded']
wt = jsonData['waitTime'] #seconds between each search
debugOutput = jsonData['debugOutput']

# Create DL folder in case it doesn't exist
try:
    os.mkdir(DLfolder)
    print('DL folder created')
except FileExistsError:
    print('DL folder already exists')

# open CSV file for writing, using DictWriter so will need DictReader later
with open(resultsLedgerPath, 'w', newline='') as csvFile:
    fieldNames = ['filename', 'title', 'snippet', 'href']
    # original order and searched query can be inferred by the filename (webpage)
    theWriter = csv.DictWriter(csvFile, fieldnames = fieldNames)

    theWriter.writeheader()

    # open the csv file with the queries for reading
    original_queries_f = open(queriesFile, 'r')
    original_queries_reader = csv.reader(original_queries_f)

    # href: queries
    pageDict = {}
    # filePath: [{query, rank}]
    filesDownloadedJSON = {}
    
    for row in original_queries_reader:
        originalQuery = row[0]
        
        print('\nExtrapolating original query: ' + row[0])

        antonyms = antGalore3.ants(originalQuery)
        
        # dict of all relavent words
        nQueries = {}
        
        for word in originalQuery.split(' '):
            nQueries[word] = []
            nQueries[word].append(word)
        
        for word in antonyms:
            if antonyms[word] != []:
                for newWord in antonyms[word]:
                    nQueries[word].append(newWord)
        
        # iterate based on query, dictionary doesn't keep order
        
        # list of all relavent words in order
        queriesExtra = []
        for word in originalQuery.split(' '):
            queriesExtra.append(nQueries[word])

        # list of all possible extrapolated queries based on original query
        possibleQueries = list(itertools.product(*queriesExtra))
        
        for queryList in possibleQueries:
            query = ' '.join(queryList)
            query = urllib.parse.quote_plus(query)

            print('\nGoogling query: ' + query)

            googlePages = 0
            # for each result in page, keep important information
            counter = 1
            # did we find something useful in the page
            foundSomething = True

            while counter <= numOfRes and foundSomething:

                foundSomething = False

                # base google search url 
                # q = query
                # num = results per page
                # start = how many results to skip ((page of results - 1) * results per page)
                url = 'https://google.com/search?filter=0&q=' + query + '&num=' + str(100) + '&start=' + str(100 * googlePages)

                # get response from google
                try:
                    now = time.time()
                    if (now - lastCall) < wt:
                        print('Sleeping for ' + str(wt - (now - lastCall)) + ' seconds')
                        time.sleep(wt - (now - lastCall))
                    print('Getting url: ' + url)
                    response = requests.get(url)
                    lastCall = time.time()
                except requests.exceptions.RequestException as e:
                    print('\nGoogle is not returning results')
                    print(str(type(e)))
                    print('Exiting now')
                    sys.exit(1)
                except:
                    print('\nGoogle is not returning results, we don\'t know why')
                    print('Exiting now')
                    sys.exit(1)

                # use beautiful soup to process the results page
                web_soup = soup(response.text, 'html.parser')

                # check if we ran out of results
                if 'did not match any documents.' in response.text:
                    if debugOutput:
                        print('\nWe run out of results for: ' + query)
                        print('Moving on to next query')
                    break

                # check if we got captchaed
                if 'Our systems have detected unusual traffic from your computer network' in str(web_soup):
                    print('\nDebug: ' + str(web_soup))
                    print('We got captchaed, exiting now')
                    sys.exit(1)

                for res in web_soup.find_all(class_='g'):
                    if res.h3 and res.h3.a:
                        href = res.h3.a['href'][7:]
                        # let's unwrap useful info
                        if href.startswith('h'):
                            ends = href.find('&')
                            href = href[:ends]
                            title = res.h3.text
                            snippet = res.find(class_='st').text

                        if href[0:4] != 'http':
                            href = 'http://' + href

                        # google books come back as https:/// for some reason
                        href = href.replace('///', '//')
                            

                        # download the landing page of the result and write it to a file
                        #print('Downloading: ' + href)
                        # get the extension if available, assumed html otherwise
                        site, ext = os.path.splitext(href)

                        # file types we are interested in
                        # Maybe try requests.head but it doesn't seem reliable enough
                        if any(ext in e for e in supportedExtensions):

                            if ext == '':
                                ext = defaultExt

                            # Input for filesDownloadedJSON
                            newInput = {
                                'query': query,
                                'rank': counter
                            }

                            # pageDict is a dictionary of different queries pointing
                            # to the same page
                            # href key
                            # all queries resulting in href in an array
                            if href in pageDict:
                                pageDict[href].append(query + str(counter) + ext)
                                filesDownloadedJSON[pageDict[href][0]]['queries'].append(copy.deepcopy(newInput))
                                foundSomething = True
                                continue

                            try:
                                new_response = requests.get(href, stream=True, timeout=6.1)
                                if int(new_response.headers['content-length']) > minFileSize and int(new_response.headers['content-length']) < maxFileSize: # if min < file < MAX
                                    new_response = requests.get(href, stream=True, timeout=6.1)
                                else:
                                    raise AssertionError('The file was either too big or too small.')
                            except KeyError as err:
                                try:
                                    new_response = requests.get(href, stream=True, timeout=6.1)
                                except requests.exceptions.RequestException as e:
                                    if debugOutput:
                                        print('\nGot e: ' + str(type(e)))
                                        print(href)
                                    continue
                            except requests.exceptions.RequestException as e:
                                if debugOutput:
                                    print('\nGot e: ' + str(type(e)))
                                    print(href)
                                continue
                            except AssertionError as err:
                                #print('Did not file at: ' + href + ' because size was: ' + new_response.headers['content-length'] + 'bytes')
                                continue
                            except Exception as exc:
                                if debugOutput:
                                    print('\nGot an exception of type: ' + str(type(exc)))
                                continue
                        else:
                            continue

                        webPageFileName = DLfolder + '/' + query + str(counter) + ext

                        try:
                            new_file = open(webPageFileName, 'wb')
                            size = 0
                            tooMuch = False
                            for chunk in new_response.iter_content(chunk_size=chunkSize):
                                size = size = chunkSize
                                if size > maxFileSize:
                                    tooMuch = True
                                    break
                                new_file.write(chunk)
                            new_file.close()

                            if tooMuch:
                                print('\nDeleting big file', flush=True)
                                os.remove(webPageFileName)
                                continue
                        except:
                            if debugOutput:
                                print('\nCould not write to file from: ' + href)
                            continue
                                
                        # write new row to csv file
                        theWriter.writerow({'filename' : webPageFileName, 'title' : title, 'snippet' : snippet, 'href' : href})


                        # Everything worked out ok

                        pageDict[href] = []
                        pageDict[href].append(webPageFileName) 

                        # Input for filesDownloadedJSON
                        newInput = {
                            'query': query,
                            'rank': counter
                        }

                        # webPageFilePath: {href, [{query, rank}]}
                        filesDownloadedJSON[webPageFileName] = {
                            'href': href,
                            'queries': [copy.deepcopy(newInput)]
                        }

                        foundSomething = True
                        counter = counter + 1
                        print('.', end='', flush=True)
                        if counter > numOfRes:
                            break
                    else:
                        continue
                if counter <= numOfRes:
                    print('\nGoing to next Google Results page')
                    googlePages = googlePages + 1
    original_queries_f.close()
    print('')
    try:
        new_file = open(pageDictPath, 'w')
        new_file.write(json.dumps(pageDict, indent=4, separators=(',', ': ')))
        new_file.close()
        new_file2 = open(filesDownloadedPath, 'w')
        new_file2.write(json.dumps(filesDownloadedJSON, indent=4, separators=(',', ': ')))
    except Exception as e:
        print('\nHad a problem with writing dict files')
        print(str(type(e)))
        sys.exit(1)
