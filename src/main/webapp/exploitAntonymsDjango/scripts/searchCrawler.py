import urllib
from bs4 import BeautifulSoup as soup
import requests
import webbrowser
import csv
import antGalore3
import itertools
import os
import json
import sys

if len(sys.argv) != 2:
    print("Usage: python3 searchCrawler.py confFile")
    sys.exit(1)

# import conf file and load json data
jsonData = json.loads(open(sys.argv[1]).read())
queriesFile = jsonData["queriesCSVfile"]
numOfRes = jsonData["numOfResPerQuery"]
DLfolder = jsonData["path2folder"]
maxFileSize = jsonData['maxFileSize']
minFileSize = jsonData['minFileSize']
dlAnyway = jsonData['dlAnyway']
chunkSize = jsonData['chunkSize']
defaultExt = jsonData['defaultExt']
resultsLedgerPath = jsonData['resultsLedger']
pageDictPath = jsonData['pageDict']
supportedExtensions = jsonData['extensions']

# Create DL folder in case it doesn't exist
try:
    os.mkdir(DLfolder)
    print('DL folder created')
except FileExistsError:
    print('DL folder already exists')

# open CSV file for writing, using DictWriter so will need DictReader later
with open(resultsLedgerPath, 'w', newline='') as csvFile:
    fieldNames = ['filename', 'title', 'snippet', 'href']
    # original order and searched query can be inferred by the filename (webpage)
    theWriter = csv.DictWriter(csvFile, fieldnames = fieldNames)

    theWriter.writeheader()

    # open the csv file with the queries for reading
    original_queries_f = open(queriesFile, 'r')
    original_queries_reader = csv.reader(original_queries_f)

    # dict of downloaded pages
    pageDict = {}
    
    for row in original_queries_reader:
        originalQuery = row[0]
        
        print('Extrapolating original query: ' + row[0])

        antonyms = antGalore3.ants(originalQuery)
        
        # dict of all relavent words
        nQueries = {}
        
        for word in originalQuery.split(' '):
            nQueries[word] = []
            nQueries[word].append(word)
        
        for word in antonyms:
            if antonyms[word] != []:
                for newWord in antonyms[word]:
                    nQueries[word].append(newWord)
        
        # iterate based on query, dictionary doesn't keep order
        
        # list of all relavent words in order
        queriesExtra = []
        for word in originalQuery.split(' '):
            queriesExtra.append(nQueries[word])

        # list of all possible extrapolated queries based on original query
        possibleQueries = list(itertools.product(*queriesExtra))
        
        for queryList in possibleQueries:
            query = ' '.join(queryList)
            query = urllib.parse.quote_plus(query)

            print('Googling query: ' + query)

            googlePages = 0
            # for each result in page, keep important information
            counter = 1

            while counter <= numOfRes:
                # base google search url 
                # q = query
                # num = results per page
                # start = how many results to skip ((page of results - 1) * results per page)
                url = 'https://google.com/search?filter=0&q=' + query + '&num=' + str(100) + '&start=' + str(100 * googlePages)

                # get response from google
                try:
                    response = requests.get(url)
                except requests.exceptions.RequestException as e:
                    print('Google is not returning results')
                    print(str(type(e)))
                    break

                # use beautiful soup to process the results page
                web_soup = soup(response.text, 'html.parser')

                for res in web_soup.find_all(class_='g'):
                    #print('-----')
                    if res.h3 and res.h3.a:
                        href = res.h3.a['href'][7:]
                        # let's unwrap useful info
                        if href.startswith('h'):
                            ends = href.find('&')
                            href = href[:ends]
                            title = res.h3.text
                            snippet = res.find(class_='st').text

                        if href[0:4] != 'http':
                            href = 'http://' + href

                        # pageDict is a dictionary of different queries pointing
                        # to the same page
                        # href key
                        # all queries resulting in href (could be duplicates)
                        if href in pageDict:
                            pageDict[href] = pageDict[href] + ' ' + query
                            continue

                        # download the landing page of the result and write it to a file
                        #print('Downloading: ' + href)
                        # get the extension if available, assumed html otherwise
                        site, ext = os.path.splitext(href)

                        # file types we are interested in
                        # Maybe try requests.head but it doesn't seem reliable enough
                        if any(ext in e for e in supportedExtensions):
                            try:
                                new_response = requests.get(href, stream=True, timeout=3.05)
                                if int(new_response.headers['content-length']) > minFileSize and int(new_response.headers['content-length']) < maxFileSize: # if min < file < MAX
                                    new_response = requests.get(href, stream=True, timeout=3.05)
                                else:
                                    raise AssertionError('The file was either too big or too small.')
                            except KeyError as err:
                                #print('Content-length key not found. Downloading anyway: ' + str(dlAnyway))
                                if dlAnyway:
                                    try:
                                        new_response = requests.get(href, stream=True, timeout=3.05)
                                    except requests.exceptions.RequestException as e:
                                        print('Got e: ' + str(type(e)) + ' http exception')
                                else:
                                    continue
                            except requests.exceptions.RequestException as e:
                                print('Got e: ' + str(type(e)) + ' http exception')
                            except AssertionError as err:
                                #print('Did not file at: ' + href + ' because size was: ' + new_response.headers['content-length'] + 'bytes')
                                continue
                            except Exception as exc:
                                #print('Got an exception of type: ' + str(type(exc)))
                                continue
                        else:
                            continue

                        if ext == '':
                            ext = '.' + defaultExt

                        webPageFileName = DLfolder + '/' + query + str(counter) + ext

                        try:
                            new_file = open(webPageFileName, 'wb')
                            size = 0
                            tooMuch = False
                            for chunk in new_response.iter_content(chunk_size=chunkSize):
                                size = size = chunkSize
                                if size > maxFileSize:
                                    tooMuch = True
                                    break
                                new_file.write(chunk)
                            new_file.close()

                            if tooMuch:
                                print('Deleting big file', flush=True)
                                os.remove(webPageFileName)
                                continue
                        except:
                            print('Could not write to file from: ' + href)
                            continue
                                
                        # write new row to csv file
                        theWriter.writerow({'filename' : webPageFileName, 'title' : title, 'snippet' : snippet, 'href' : href})
                                
                        # Everything worked out ok
                        pageDict[href] = query 
                        counter = counter + 1
                        print('.', end='', flush=True)
                        if counter > numOfRes:
                            break
                    else:
                        continue
                googlePages = googlePages + 1
    original_queries_f.close()
    new_file = open(pageDictPath, 'w')
    new_file.write(json.dumps(pageDict))
    new_file.close()
