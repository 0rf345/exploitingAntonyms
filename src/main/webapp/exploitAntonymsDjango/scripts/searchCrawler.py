import urllib
from bs4 import BeautifulSoup as soup
import requests
import webbrowser
import csv
import antGalore3
import itertools

# open CSV file for writing, using DictWriter so will need DictReader later
with open('resultsLedger.csv', 'w', newline='') as csvFile:
    fieldNames = ['filename', 'title', 'snippet', 'href']
    # original order and searched query can be inferred by the filename (webpage)
    theWriter = csv.DictWriter(csvFile, fieldnames = fieldNames)

    theWriter.writeheader()

    originalQuery = 'Good uses for aloe'
    antonyms = antGalore3.ants(originalQuery)
    
    # dict of all relavent words
    nQueries = {}
    
    for word in originalQuery.split(' '):
        nQueries[word] = []
        nQueries[word].append(word)
    
    for word in antonyms:
        if antonyms[word] != []:
            for newWord in antonyms[word]:
                nQueries[word].append(newWord)
    
    # iterate based on query, dictionary doesn't keep order
    
    # list of all relavent words in order
    queriesExtra = []
    for word in originalQuery.split(' '):
        queriesExtra.append(nQueries[word])
    
    # list of all possible extrapolated queries based on original query
    possibleQueries = list(itertools.product(*queriesExtra))
    
    for queryList in possibleQueries:
        query = ' '.join(queryList)
        query = urllib.parse.quote_plus(query)

        # number of results we want
        numOfRes = 100

        # base google search url + query + number of results we want
        url = 'https://google.com/search?q=' + query + '&num=' + str(numOfRes)

        # get response from google
        response = requests.get(url)

        # use beautiful soup to process the results page
        web_soup = soup(response.text, 'html')

        
        # for each result in page, keep important information
        counter = 1
        for res in web_soup.find_all(class_='g'):
            #print('-----')
            if res.h3:
                    href = res.h3.a['href'][7:]
                    # let's unwrap useful info
                    if href.startswith('h'):
                        ends = href.find('&')
                        href = href[:ends]
                        title = res.h3.text
                        snippet = res.find(class_='st').text
                        #print('Title: ' + title)
                        #print('Link: ' + href)
                        #print('Snippet: ' + snippet)

                        # download the landing page of the result and write it to a file
                        print('Downloading: ' + href)
                        new_response = requests.get(href)
                        print('Downloaded: ' + href)
                        webPageFileName = query + str(counter) + '.html'
                        new_file = open(webPageFileName, 'w')
                        new_file.write(new_response.text)
                        new_file.close()
                        
                        # write new row to csv file
                        theWriter.writerow({'filename' : webPageFileName, 'title' : title, 'snippet' : snippet, 'href' : href})
                        
                        #print('Wrote to file: '+ query+str(counter))
                        counter = counter + 1
                    else:
                        continue
                        